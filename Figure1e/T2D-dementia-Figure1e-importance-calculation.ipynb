{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16a3a3f-a1a7-4826-bdbb-d5a0e25fc241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 数据加载与预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Best configuration: {'layers': [1024, 512, 256, 128], 'dropout': 0.2, 'l2': 0.01, 'lr': 0.0001} with validation MAE: 0.3442367601246106\n",
      "\n",
      "3. 模型训练中...\n",
      "Epoch 1/30\n",
      "502/502 - 9s - loss: 19.5711 - accuracy: 0.1941 - val_loss: 11.2615 - val_accuracy: 0.2046 - 9s/epoch - 19ms/step\n",
      "Epoch 2/30\n",
      "502/502 - 8s - loss: 14.7271 - accuracy: 0.1885 - val_loss: 8.8981 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "502/502 - 8s - loss: 12.2302 - accuracy: 0.1913 - val_loss: 7.3920 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 4/30\n",
      "502/502 - 8s - loss: 10.5282 - accuracy: 0.1944 - val_loss: 6.4423 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 5/30\n",
      "502/502 - 8s - loss: 9.3302 - accuracy: 0.1956 - val_loss: 5.8228 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 6/30\n",
      "502/502 - 8s - loss: 8.4634 - accuracy: 0.1966 - val_loss: 5.2328 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 7/30\n",
      "502/502 - 8s - loss: 7.8279 - accuracy: 0.1986 - val_loss: 4.8468 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 8/30\n",
      "502/502 - 8s - loss: 7.2729 - accuracy: 0.1987 - val_loss: 4.5137 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 9/30\n",
      "502/502 - 8s - loss: 6.8240 - accuracy: 0.1988 - val_loss: 4.2676 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "502/502 - 8s - loss: 6.4894 - accuracy: 0.1992 - val_loss: 4.0733 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 11/30\n",
      "502/502 - 8s - loss: 6.1700 - accuracy: 0.1994 - val_loss: 3.9121 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 12/30\n",
      "502/502 - 8s - loss: 5.9163 - accuracy: 0.1998 - val_loss: 3.7686 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 13/30\n",
      "502/502 - 8s - loss: 5.6738 - accuracy: 0.1997 - val_loss: 3.6874 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 14/30\n",
      "502/502 - 8s - loss: 5.4915 - accuracy: 0.1996 - val_loss: 3.5858 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 15/30\n",
      "502/502 - 8s - loss: 5.3222 - accuracy: 0.1998 - val_loss: 3.5220 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 16/30\n",
      "502/502 - 8s - loss: 5.1818 - accuracy: 0.1998 - val_loss: 3.4371 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 17/30\n",
      "502/502 - 8s - loss: 5.0247 - accuracy: 0.2000 - val_loss: 3.4390 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 18/30\n",
      "502/502 - 8s - loss: 4.9106 - accuracy: 0.2000 - val_loss: 3.3780 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 19/30\n",
      "502/502 - 8s - loss: 4.8228 - accuracy: 0.2000 - val_loss: 3.3835 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 20/30\n",
      "502/502 - 8s - loss: 4.6970 - accuracy: 0.1999 - val_loss: 3.3594 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 21/30\n",
      "502/502 - 8s - loss: 4.6433 - accuracy: 0.2000 - val_loss: 3.3158 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 22/30\n",
      "502/502 - 8s - loss: 4.5267 - accuracy: 0.2000 - val_loss: 3.3062 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 23/30\n",
      "502/502 - 8s - loss: 4.4871 - accuracy: 0.2000 - val_loss: 3.3430 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 24/30\n",
      "502/502 - 6s - loss: 4.4105 - accuracy: 0.2000 - val_loss: 3.3205 - val_accuracy: 0.2046 - 6s/epoch - 13ms/step\n",
      "Epoch 25/30\n",
      "502/502 - 8s - loss: 4.3385 - accuracy: 0.2000 - val_loss: 3.3116 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 26/30\n",
      "502/502 - 9s - loss: 4.2758 - accuracy: 0.2000 - val_loss: 3.3343 - val_accuracy: 0.2046 - 9s/epoch - 18ms/step\n",
      "Epoch 27/30\n",
      "502/502 - 9s - loss: 4.2304 - accuracy: 0.2000 - val_loss: 3.2978 - val_accuracy: 0.2046 - 9s/epoch - 17ms/step\n",
      "Epoch 28/30\n",
      "502/502 - 9s - loss: 4.1814 - accuracy: 0.2000 - val_loss: 3.3074 - val_accuracy: 0.2046 - 9s/epoch - 18ms/step\n",
      "Epoch 29/30\n",
      "502/502 - 9s - loss: 4.1150 - accuracy: 0.2000 - val_loss: 3.3738 - val_accuracy: 0.2046 - 9s/epoch - 18ms/step\n",
      "Epoch 30/30\n",
      "502/502 - 9s - loss: 4.0745 - accuracy: 0.2000 - val_loss: 3.3459 - val_accuracy: 0.2043 - 9s/epoch - 18ms/step\n",
      "\n",
      "4. 模型评估...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59       816\n",
      "           1       0.49      0.45      0.47      1560\n",
      "           2       0.35      0.43      0.39      1009\n",
      "           3       0.39      0.35      0.37       495\n",
      "           4       0.18      0.08      0.11       109\n",
      "\n",
      "    accuracy                           0.45      3989\n",
      "   macro avg       0.40      0.38      0.39      3989\n",
      "weighted avg       0.46      0.45      0.45      3989\n",
      "\n",
      "MAE: 0.6906492855352219\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "from scipy.stats import bootstrap\n",
    "import numpy as np \n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================= T2D-dementia Fluctuating_Decreasing cluster =============================\n",
    "\n",
    "# =============================== 配置区 ===============================\n",
    "DATA_PATH = \"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Fluctuating_Decreasing.csv\" \n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-5 \n",
    "\n",
    "# ============================= 1.数据准备 =============================\n",
    "print(\"\\n1. 数据加载与预处理...\")\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Fluctuating_Decreasing.csv\")\n",
    "protein_names = df.columns[9:76].tolist()\n",
    "covariates_names = df.columns[2:9].tolist()\n",
    "num_proteins = len(protein_names)\n",
    "\n",
    "# 数据拆分\n",
    "y = df.iloc[:, 1].values.astype(np.int32)\n",
    "X_protein = df.iloc[:, 9:76].values.astype(np.float32)\n",
    "X_covariates = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# 缺失值处理\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_protein = imputer.fit_transform(X_protein)\n",
    "X_covariates = imputer.fit_transform(X_covariates)\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_protein_scaled = scaler.fit_transform(X_protein)\n",
    "X_covariates_scaled = scaler.fit_transform(X_covariates)\n",
    "X_combined = np.hstack([X_protein_scaled, X_covariates_scaled])\n",
    "\n",
    "# 数据集拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# SMOTE过采样\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Further split training data into sub-train and validation for grid search\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define grid search parameters\n",
    "layer_configs = [[1024, 512, 256, 128], [512, 256, 128], [256, 128]]  # Varying complexity\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "l2_lambdas = [0.001, 0.01, 0.1]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Function to build model with variable hyperparameters\n",
    "class MonotonicConstraint(tf.keras.constraints.Constraint):\n",
    "    \"\"\"强制阈值参数单调递增约束\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.cumsum(tf.nn.elu(w) + 1e-6)\n",
    "\n",
    "def build_model(input_dim, num_classes, layer_sizes, dropout_rate, l2_lambda, learning_rate):\n",
    "    class DeepOrdinal(tf.keras.Model):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # 网络结构\n",
    "            self.dense_stack = tf.keras.Sequential()\n",
    "            for size in layer_sizes:\n",
    "                self.dense_stack.add(tf.keras.layers.Dense(size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "                self.dense_stack.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "            self.output_layer = tf.keras.layers.Dense(num_classes-1, kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))\n",
    "            \n",
    "            # 可训练阈值参数\n",
    "            self.thresholds = tf.Variable(\n",
    "                initial_value=tf.sort(tf.linspace(-1.0, 1.0, num_classes-1)),\n",
    "                trainable=True,\n",
    "                constraint=MonotonicConstraint(),\n",
    "                name=\"ordinal_thresholds\"\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_stack(inputs)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            \"\"\"序数回归损失函数\"\"\"\n",
    "            y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)\n",
    "            cum_loss = 0.0\n",
    "            \n",
    "            for k in range(self.num_classes-1):\n",
    "                target = tf.cast(y_true > k, tf.float32)\n",
    "                logit = y_pred[:, k] - self.thresholds[k]\n",
    "                \n",
    "                # 添加数值稳定化\n",
    "                logit = tf.clip_by_value(logit, -10.0, 10.0)\n",
    "                logit = tf.reshape(logit, (-1, 1))\n",
    "                \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(target, logit)\n",
    "                cum_loss += tf.reduce_mean(loss)\n",
    "                \n",
    "            return cum_loss\n",
    "\n",
    "    model = DeepOrdinal(input_dim=input_dim, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipvalue=1.0  # 梯度裁剪\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.custom_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "best_score = float('inf')  # Minimize validation MAE\n",
    "best_params = None\n",
    "results = []  # Track all configurations\n",
    "\n",
    "for layer_sizes, dropout_rate, l2_lambda, lr in itertools.product(layer_configs, dropout_rates, l2_lambdas, learning_rates):\n",
    "    print(f\"Evaluating config: layers={layer_sizes}, dropout={dropout_rate}, l2={l2_lambda}, lr={lr}\")\n",
    "    \n",
    "    model = build_model(X_subtrain.shape[1], num_classes=5, layer_sizes=layer_sizes, \n",
    "                        dropout_rate=dropout_rate, l2_lambda=l2_lambda, learning_rate=lr)\n",
    "    \n",
    "    # Train with class weights\n",
    "    history = model.fit(\n",
    "        X_subtrain, y_subtrain,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weight,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict on validation and compute MAE\n",
    "    y_val_pred = ordinal_predict(model, X_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr, 'val_mae': val_mae\n",
    "    })\n",
    "    \n",
    "    if val_mae < best_score:\n",
    "        best_score = val_mae\n",
    "        best_params = {'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr}\n",
    "\n",
    "# Save results to CSV for review\n",
    "pd.DataFrame(results).to_csv('E:/neuro_od/T2D-痴呆结果/grid_search_results.csv', index=False)\n",
    "\n",
    "print(f\"Best configuration: {best_params} with validation MAE: {best_score}\")\n",
    "\n",
    "# Now, use best_params to build and train the final model on full X_train, y_train\n",
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=5,\n",
    "    layer_sizes=best_params['layers'],\n",
    "    dropout_rate=best_params['dropout'],\n",
    "    l2_lambda=best_params['l2'],\n",
    "    learning_rate=best_params['lr']\n",
    ")\n",
    "\n",
    "# ============================ 3.模型训练 =============================\n",
    "print(\"\\n3. 模型训练中...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# =========================== 4.模型评估 =============================\n",
    "print(\"\\n4. 模型评估...\")\n",
    "\n",
    "y_pred = ordinal_predict(model, X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46df7fa7-1a73-41cc-9312-5f84d12d4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. 计算梯度重要性...\n",
      "Length of protein_names: 67\n",
      "Shape of gradient_importance_mean: (67,)\n",
      "Shape of gradient_importance_se: (67,)\n",
      "Shape of gradient_importance_ci_low: (67,)\n",
      "Shape of gradient_importance_ci_high: (67,)\n"
     ]
    }
   ],
   "source": [
    "# ===================== 5.梯度重要性计算（蛋白质部分）=====================\n",
    "print(\"\\n5. 计算梯度重要性...\")\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)  # 显式监视输入\n",
    "        preds = model(inputs)\n",
    "    return tape.gradient(preds, inputs)\n",
    "\n",
    "# 计算所有训练样本的梯度均值\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "gradients = compute_gradients(X_train_tensor)\n",
    "\n",
    "if gradients is None:\n",
    "    raise ValueError(\"梯度计算失败，请检查模型输入输出依赖关系\")\n",
    "\n",
    "# 调整聚合维度（假设输入为二维张量）\n",
    "abs_gradients = tf.abs(gradients).numpy()  # Convert to NumPy for bootstrapping\n",
    "protein_abs_gradients = abs_gradients[:, :num_proteins]  # Extract protein part (first num_proteins columns)\n",
    "\n",
    "# Bootstrapping function for mean importance per protein\n",
    "def bootstrap_mean(data):\n",
    "    return np.mean(data, axis=0)  # Mean across samples for each protein\n",
    "\n",
    "# Perform bootstrapping (95% CI, 1000 resamples)\n",
    "boot_result = bootstrap((protein_abs_gradients,), bootstrap_mean, n_resamples=1000, random_state=RANDOM_STATE, method='percentile')\n",
    "\n",
    "# Extract statistics\n",
    "gradient_importance_mean = boot_result.bootstrap_distribution.mean(axis=-1)  # Mean of bootstraps across resamples; results in shape (67,)\n",
    "gradient_importance_se = boot_result.standard_error\n",
    "gradient_importance_ci_low = boot_result.confidence_interval.low\n",
    "gradient_importance_ci_high = boot_result.confidence_interval.high\n",
    "\n",
    "# Diagnostic prints to verify lengths (optional; can be removed after confirmation)\n",
    "print(\"Length of protein_names:\", len(protein_names))\n",
    "print(\"Shape of gradient_importance_mean:\", gradient_importance_mean.shape)\n",
    "print(\"Shape of gradient_importance_se:\", gradient_importance_se.shape)\n",
    "print(\"Shape of gradient_importance_ci_low:\", gradient_importance_ci_low.shape)\n",
    "print(\"Shape of gradient_importance_ci_high:\", gradient_importance_ci_high.shape)\n",
    "\n",
    "# Save results to CSV\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': gradient_importance_mean,\n",
    "    'Gradient Importance SE': gradient_importance_se,\n",
    "    'Gradient Importance CI Lower (95%)': gradient_importance_ci_low,\n",
    "    'Gradient Importance CI Upper (95%)': gradient_importance_ci_high\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_dementia_Fluctuating_Decreasing_深度有序回归_with_ci.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfa7e2-f1fe-4fbb-95f9-340753e5e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Permutation Test Parameters\n",
    "n_permutations = 1000  # Adjust for computational feasibility\n",
    "\n",
    "# Function to compute mean absolute gradients\n",
    "def compute_mean_abs_gradients(inputs):\n",
    "    grads = compute_gradients(tf.convert_to_tensor(inputs, dtype=tf.float32))\n",
    "    return np.mean(np.abs(grads.numpy()), axis=0)[:num_proteins]  # Protein part only\n",
    "\n",
    "# Original mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Generate null distribution\n",
    "null_distribution = np.zeros((n_permutations, num_proteins))\n",
    "for i in range(n_permutations):\n",
    "    # Permute features (columns) independently for each protein\n",
    "    permuted_X = X_train.copy()\n",
    "    for j in range(num_proteins):\n",
    "        np.random.shuffle(permuted_X[:, j])\n",
    "    null_distribution[i] = compute_mean_abs_gradients(permuted_X)\n",
    "\n",
    "# Compute p-values (one-tailed: proportion of null means >= original)\n",
    "p_values_perm = np.array([1 - (percentileofscore(null_distribution[:, j], original_mean[j]) / 100) for j in range(num_proteins)])\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': original_mean,\n",
    "    'P-value (Permutation)': p_values_perm\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_permutation_test_Fluctuating_Decreasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb0b8b-1261-4ad2-8b2b-56b4b1a8d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified; insert after model training\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Approximate pivotal statistic (e.g., standardized gradient mean)\n",
    "std_gradients = np.std(protein_abs_gradients, axis=0)\n",
    "pivotal_stats = original_mean / (std_gradients / np.sqrt(protein_abs_gradients.shape[0]))\n",
    "p_values_pivotal = 2 * (1 - norm.cdf(np.abs(pivotal_stats)))  # Two-tailed\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Pivotal Statistic': pivotal_stats,\n",
    "    'P-value (Pivotal)': p_values_pivotal\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_pivotal_test_Fluctuating_Decreasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91e984b-b5c3-45ea-baff-6c154528292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 数据加载与预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Best configuration: {'layers': [1024, 512, 256, 128], 'dropout': 0.4, 'l2': 0.01, 'lr': 5e-05} with validation MAE: 0.30778816199376946\n",
      "\n",
      "3. 模型训练中...\n",
      "Epoch 1/30\n",
      "502/502 - 10s - loss: 28.9207 - accuracy: 0.1987 - val_loss: 19.4287 - val_accuracy: 0.2046 - 10s/epoch - 20ms/step\n",
      "Epoch 2/30\n",
      "502/502 - 8s - loss: 24.9016 - accuracy: 0.1923 - val_loss: 18.0533 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "502/502 - 7s - loss: 22.4996 - accuracy: 0.1912 - val_loss: 16.9171 - val_accuracy: 0.2046 - 7s/epoch - 14ms/step\n",
      "Epoch 4/30\n",
      "502/502 - 8s - loss: 20.8413 - accuracy: 0.1889 - val_loss: 15.7945 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 5/30\n",
      "502/502 - 7s - loss: 19.3805 - accuracy: 0.1902 - val_loss: 14.7721 - val_accuracy: 0.2046 - 7s/epoch - 15ms/step\n",
      "Epoch 6/30\n",
      "502/502 - 7s - loss: 18.0475 - accuracy: 0.1904 - val_loss: 13.7737 - val_accuracy: 0.2046 - 7s/epoch - 15ms/step\n",
      "Epoch 7/30\n",
      "502/502 - 7s - loss: 16.8484 - accuracy: 0.1918 - val_loss: 12.8722 - val_accuracy: 0.2046 - 7s/epoch - 14ms/step\n",
      "Epoch 8/30\n",
      "502/502 - 8s - loss: 15.7229 - accuracy: 0.1932 - val_loss: 12.0437 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 9/30\n",
      "502/502 - 7s - loss: 14.6883 - accuracy: 0.1945 - val_loss: 11.2597 - val_accuracy: 0.2046 - 7s/epoch - 15ms/step\n",
      "Epoch 10/30\n",
      "502/502 - 8s - loss: 13.7394 - accuracy: 0.1952 - val_loss: 10.5603 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 11/30\n",
      "502/502 - 7s - loss: 12.9038 - accuracy: 0.1954 - val_loss: 9.9634 - val_accuracy: 0.2046 - 7s/epoch - 14ms/step\n",
      "Epoch 12/30\n",
      "502/502 - 8s - loss: 12.1686 - accuracy: 0.1969 - val_loss: 9.4007 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 13/30\n",
      "502/502 - 8s - loss: 11.4687 - accuracy: 0.1974 - val_loss: 8.9187 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 14/30\n",
      "502/502 - 7s - loss: 10.8342 - accuracy: 0.1981 - val_loss: 8.4566 - val_accuracy: 0.2046 - 7s/epoch - 15ms/step\n",
      "Epoch 15/30\n",
      "502/502 - 8s - loss: 10.2685 - accuracy: 0.1980 - val_loss: 8.0448 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 16/30\n",
      "502/502 - 8s - loss: 9.7791 - accuracy: 0.1988 - val_loss: 7.6722 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 17/30\n",
      "502/502 - 8s - loss: 9.3067 - accuracy: 0.1991 - val_loss: 7.3147 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 18/30\n",
      "502/502 - 7s - loss: 8.8779 - accuracy: 0.1993 - val_loss: 7.0068 - val_accuracy: 0.2046 - 7s/epoch - 15ms/step\n",
      "Epoch 19/30\n",
      "502/502 - 8s - loss: 8.4781 - accuracy: 0.1993 - val_loss: 6.7317 - val_accuracy: 0.2046 - 8s/epoch - 15ms/step\n",
      "Epoch 20/30\n",
      "502/502 - 8s - loss: 8.1111 - accuracy: 0.1993 - val_loss: 6.4773 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 21/30\n",
      "502/502 - 7s - loss: 7.7773 - accuracy: 0.1995 - val_loss: 6.2251 - val_accuracy: 0.2046 - 7s/epoch - 14ms/step\n",
      "Epoch 22/30\n",
      "502/502 - 8s - loss: 7.4774 - accuracy: 0.1997 - val_loss: 6.0075 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 23/30\n",
      "502/502 - 7s - loss: 7.1956 - accuracy: 0.1997 - val_loss: 5.8064 - val_accuracy: 0.2046 - 7s/epoch - 14ms/step\n",
      "Epoch 24/30\n",
      "502/502 - 7s - loss: 8.5395 - accuracy: 0.2121 - val_loss: 6.1422 - val_accuracy: 0.2592 - 7s/epoch - 15ms/step\n",
      "Epoch 25/30\n",
      "502/502 - 8s - loss: 23.2693 - accuracy: 0.2614 - val_loss: 8.1214 - val_accuracy: 0.2873 - 8s/epoch - 16ms/step\n",
      "Epoch 26/30\n",
      "502/502 - 8s - loss: 34.4673 - accuracy: 0.2167 - val_loss: 7.7854 - val_accuracy: 0.4001 - 8s/epoch - 16ms/step\n",
      "Epoch 27/30\n",
      "502/502 - 8s - loss: 35.2160 - accuracy: 0.2316 - val_loss: 7.9878 - val_accuracy: 0.4039 - 8s/epoch - 15ms/step\n",
      "Epoch 28/30\n",
      "502/502 - 8s - loss: 36.4078 - accuracy: 0.2349 - val_loss: 8.3911 - val_accuracy: 0.4031 - 8s/epoch - 15ms/step\n",
      "Epoch 29/30\n",
      "502/502 - 8s - loss: 37.3070 - accuracy: 0.2351 - val_loss: 8.9248 - val_accuracy: 0.4059 - 8s/epoch - 16ms/step\n",
      "Epoch 30/30\n",
      "502/502 - 8s - loss: 38.0048 - accuracy: 0.2283 - val_loss: 8.9716 - val_accuracy: 0.3986 - 8s/epoch - 16ms/step\n",
      "\n",
      "4. 模型评估...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.65       816\n",
      "           1       0.56      0.57      0.56      1560\n",
      "           2       0.34      0.51      0.40      1009\n",
      "           3       0.00      0.00      0.00       495\n",
      "           4       0.00      0.00      0.00       109\n",
      "\n",
      "    accuracy                           0.49      3989\n",
      "   macro avg       0.31      0.35      0.32      3989\n",
      "weighted avg       0.43      0.49      0.46      3989\n",
      "\n",
      "MAE: 0.5886187014289296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================= T2D-痴呆结果 Fluctuating_Increasing =============================\n",
    "\n",
    "# =============================== 配置区 ===============================\n",
    "DATA_PATH = \"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Fluctuating_Increasing.csv\"  # 修改为实际路径\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "SHAP_SAMPLE_SIZE = 500  # SHAP计算样本量\n",
    "LEARNING_RATE = 1e-5  # 调低学习率\n",
    "\n",
    "# ============================= 1.数据准备 =============================\n",
    "print(\"\\n1. 数据加载与预处理...\")\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Fluctuating_Increasing.csv\")\n",
    "protein_names = df.columns[9:485].tolist()\n",
    "covariates_names = df.columns[2:9].tolist()\n",
    "num_proteins = len(protein_names)\n",
    "\n",
    "# 数据拆分\n",
    "y = df.iloc[:, 1].values.astype(np.int32)\n",
    "X_protein = df.iloc[:, 9:485].values.astype(np.float32)\n",
    "X_covariates = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# 缺失值处理\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_protein = imputer.fit_transform(X_protein)\n",
    "X_covariates = imputer.fit_transform(X_covariates)\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_protein_scaled = scaler.fit_transform(X_protein)\n",
    "X_covariates_scaled = scaler.fit_transform(X_covariates)\n",
    "X_combined = np.hstack([X_protein_scaled, X_covariates_scaled])\n",
    "\n",
    "# 数据集拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# SMOTE过采样\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Further split training data into sub-train and validation for grid search\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define grid search parameters\n",
    "layer_configs = [[1024, 512, 256, 128], [512, 256, 128], [256, 128]]  # Varying complexity\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "l2_lambdas = [0.001, 0.01, 0.1]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Function to build model with variable hyperparameters\n",
    "class MonotonicConstraint(tf.keras.constraints.Constraint):\n",
    "    \"\"\"强制阈值参数单调递增约束\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.cumsum(tf.nn.elu(w) + 1e-6)\n",
    "\n",
    "def build_model(input_dim, num_classes, layer_sizes, dropout_rate, l2_lambda, learning_rate):\n",
    "    class DeepOrdinal(tf.keras.Model):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # 网络结构\n",
    "            self.dense_stack = tf.keras.Sequential()\n",
    "            for size in layer_sizes:\n",
    "                self.dense_stack.add(tf.keras.layers.Dense(size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "                self.dense_stack.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "            self.output_layer = tf.keras.layers.Dense(num_classes-1, kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))\n",
    "            \n",
    "            # 可训练阈值参数\n",
    "            self.thresholds = tf.Variable(\n",
    "                initial_value=tf.sort(tf.linspace(-1.0, 1.0, num_classes-1)),\n",
    "                trainable=True,\n",
    "                constraint=MonotonicConstraint(),\n",
    "                name=\"ordinal_thresholds\"\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_stack(inputs)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            \"\"\"序数回归损失函数\"\"\"\n",
    "            y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)\n",
    "            cum_loss = 0.0\n",
    "            \n",
    "            for k in range(self.num_classes-1):\n",
    "                target = tf.cast(y_true > k, tf.float32)\n",
    "                logit = y_pred[:, k] - self.thresholds[k]\n",
    "                \n",
    "                # 添加数值稳定化\n",
    "                logit = tf.clip_by_value(logit, -10.0, 10.0)\n",
    "                logit = tf.reshape(logit, (-1, 1))\n",
    "                \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(target, logit)\n",
    "                cum_loss += tf.reduce_mean(loss)\n",
    "                \n",
    "            return cum_loss\n",
    "\n",
    "    model = DeepOrdinal(input_dim=input_dim, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipvalue=1.0  # 梯度裁剪\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.custom_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "best_score = float('inf')  # Minimize validation MAE\n",
    "best_params = None\n",
    "results = []  # Track all configurations\n",
    "\n",
    "for layer_sizes, dropout_rate, l2_lambda, lr in itertools.product(layer_configs, dropout_rates, l2_lambdas, learning_rates):\n",
    "    print(f\"Evaluating config: layers={layer_sizes}, dropout={dropout_rate}, l2={l2_lambda}, lr={lr}\")\n",
    "    \n",
    "    model = build_model(X_subtrain.shape[1], num_classes=5, layer_sizes=layer_sizes, \n",
    "                        dropout_rate=dropout_rate, l2_lambda=l2_lambda, learning_rate=lr)\n",
    "    \n",
    "    # Train with class weights\n",
    "    history = model.fit(\n",
    "        X_subtrain, y_subtrain,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weight,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict on validation and compute MAE\n",
    "    y_val_pred = ordinal_predict(model, X_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr, 'val_mae': val_mae\n",
    "    })\n",
    "    \n",
    "    if val_mae < best_score:\n",
    "        best_score = val_mae\n",
    "        best_params = {'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr}\n",
    "\n",
    "# Save results to CSV for review\n",
    "pd.DataFrame(results).to_csv('E:/neuro_od/T2D-痴呆结果/grid_search_results.csv', index=False)\n",
    "\n",
    "print(f\"Best configuration: {best_params} with validation MAE: {best_score}\")\n",
    "\n",
    "# Now, use best_params to build and train the final model on full X_train, y_train\n",
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=5,\n",
    "    layer_sizes=best_params['layers'],\n",
    "    dropout_rate=best_params['dropout'],\n",
    "    l2_lambda=best_params['l2'],\n",
    "    learning_rate=best_params['lr']\n",
    ")\n",
    "\n",
    "# ============================ 3.模型训练 =============================\n",
    "print(\"\\n3. 模型训练中...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# =========================== 4.模型评估 =============================\n",
    "print(\"\\n4. 模型评估...\")\n",
    "\n",
    "y_pred = ordinal_predict(model, X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b754bc-43fe-41ee-814d-dc6259411393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5.梯度重要性计算（蛋白质部分）=====================\n",
    "print(\"\\n5. 计算梯度重要性...\")\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)  # 显式监视输入\n",
    "        preds = model(inputs)\n",
    "    return tape.gradient(preds, inputs)\n",
    "\n",
    "# 计算所有训练样本的梯度均值\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "gradients = compute_gradients(X_train_tensor)\n",
    "\n",
    "if gradients is None:\n",
    "    raise ValueError(\"梯度计算失败，请检查模型输入输出依赖关系\")\n",
    "\n",
    "# 调整聚合维度（假设输入为二维张量）\n",
    "abs_gradients = tf.abs(gradients).numpy()  # Convert to NumPy for bootstrapping\n",
    "protein_abs_gradients = abs_gradients[:, :num_proteins]  # Extract protein part (first num_proteins columns)\n",
    "\n",
    "# Bootstrapping function for mean importance per protein\n",
    "def bootstrap_mean(data):\n",
    "    return np.mean(data, axis=0)  # Mean across samples for each protein\n",
    "\n",
    "# Perform bootstrapping (95% CI, 1000 resamples)\n",
    "boot_result = bootstrap((protein_abs_gradients,), bootstrap_mean, n_resamples=1000, random_state=RANDOM_STATE, method='percentile')\n",
    "\n",
    "# Extract statistics\n",
    "gradient_importance_mean = boot_result.bootstrap_distribution.mean(axis=-1)  # Mean of bootstraps across resamples; results in shape (67,)\n",
    "gradient_importance_se = boot_result.standard_error\n",
    "gradient_importance_ci_low = boot_result.confidence_interval.low\n",
    "gradient_importance_ci_high = boot_result.confidence_interval.high\n",
    "\n",
    "# Diagnostic prints to verify lengths (optional; can be removed after confirmation)\n",
    "print(\"Length of protein_names:\", len(protein_names))\n",
    "print(\"Shape of gradient_importance_mean:\", gradient_importance_mean.shape)\n",
    "print(\"Shape of gradient_importance_se:\", gradient_importance_se.shape)\n",
    "print(\"Shape of gradient_importance_ci_low:\", gradient_importance_ci_low.shape)\n",
    "print(\"Shape of gradient_importance_ci_high:\", gradient_importance_ci_high.shape)\n",
    "\n",
    "# Save results to CSV\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': gradient_importance_mean,\n",
    "    'Gradient Importance SE': gradient_importance_se,\n",
    "    'Gradient Importance CI Lower (95%)': gradient_importance_ci_low,\n",
    "    'Gradient Importance CI Upper (95%)': gradient_importance_ci_high\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_dementia_Fluctuating_Increasing_深度有序回归_with_ci.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd692a6b-f019-4506-8437-e9a1e5ec8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Permutation Test Parameters\n",
    "n_permutations = 1000  # Adjust for computational feasibility\n",
    "\n",
    "# Function to compute mean absolute gradients\n",
    "def compute_mean_abs_gradients(inputs):\n",
    "    grads = compute_gradients(tf.convert_to_tensor(inputs, dtype=tf.float32))\n",
    "    return np.mean(np.abs(grads.numpy()), axis=0)[:num_proteins]  # Protein part only\n",
    "\n",
    "# Original mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Generate null distribution\n",
    "null_distribution = np.zeros((n_permutations, num_proteins))\n",
    "for i in range(n_permutations):\n",
    "    # Permute features (columns) independently for each protein\n",
    "    permuted_X = X_train.copy()\n",
    "    for j in range(num_proteins):\n",
    "        np.random.shuffle(permuted_X[:, j])\n",
    "    null_distribution[i] = compute_mean_abs_gradients(permuted_X)\n",
    "\n",
    "# Compute p-values (one-tailed: proportion of null means >= original)\n",
    "p_values_perm = np.array([1 - (percentileofscore(null_distribution[:, j], original_mean[j]) / 100) for j in range(num_proteins)])\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': original_mean,\n",
    "    'P-value (Permutation)': p_values_perm\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_permutation_test_Fluctuating_increasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfac72c-dbe7-45fe-a0aa-4460a2eee5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pivotal Test\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "std_gradients = np.std(protein_abs_gradients, axis=0)\n",
    "epsilon = 1e-10\n",
    "std_gradients = np.maximum(std_gradients, epsilon)\n",
    "pivotal_stats = original_mean / (std_gradients / np.sqrt(protein_abs_gradients.shape[0]))\n",
    "p_values_pivotal = 2 * (1 - norm.cdf(np.abs(pivotal_stats)))\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Pivotal Statistic': pivotal_stats,\n",
    "    'P-value (Pivotal)': p_values_pivotal\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_pivotal_test_Fluctuating_increasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e91cf29-2c85-4bad-8030-55de4b52f858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 数据加载与预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Best configuration: {'layers': [512, 256, 128], 'dropout': 0.2, 'l2': 0.01, 'lr': 1e-05} with validation MAE: 0.4053997923156802\n",
      "\n",
      "3. 模型训练中...\n",
      "Epoch 1/30\n",
      "502/502 - 9s - loss: 20.8899 - accuracy: 0.2062 - val_loss: 11.5688 - val_accuracy: 0.2053 - 9s/epoch - 17ms/step\n",
      "Epoch 2/30\n",
      "502/502 - 8s - loss: 18.4957 - accuracy: 0.1981 - val_loss: 11.2199 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "502/502 - 8s - loss: 17.5183 - accuracy: 0.1950 - val_loss: 10.9614 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 4/30\n",
      "502/502 - 8s - loss: 16.7592 - accuracy: 0.1948 - val_loss: 10.7656 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 5/30\n",
      "502/502 - 8s - loss: 16.1099 - accuracy: 0.1923 - val_loss: 10.5864 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 6/30\n",
      "502/502 - 8s - loss: 15.6189 - accuracy: 0.1912 - val_loss: 10.4273 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 7/30\n",
      "502/502 - 8s - loss: 15.2058 - accuracy: 0.1909 - val_loss: 10.3027 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 8/30\n",
      "502/502 - 8s - loss: 14.8432 - accuracy: 0.1902 - val_loss: 10.1641 - val_accuracy: 0.2046 - 8s/epoch - 16ms/step\n",
      "Epoch 9/30\n",
      "502/502 - 8s - loss: 14.5513 - accuracy: 0.1892 - val_loss: 10.0333 - val_accuracy: 0.2033 - 8s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "502/502 - 8s - loss: 14.2520 - accuracy: 0.1877 - val_loss: 9.9147 - val_accuracy: 0.2036 - 8s/epoch - 16ms/step\n",
      "Epoch 11/30\n",
      "502/502 - 8s - loss: 13.9956 - accuracy: 0.1881 - val_loss: 9.8000 - val_accuracy: 0.2031 - 8s/epoch - 15ms/step\n",
      "Epoch 12/30\n",
      "502/502 - 5s - loss: 13.7749 - accuracy: 0.1846 - val_loss: 9.6906 - val_accuracy: 0.2021 - 5s/epoch - 11ms/step\n",
      "Epoch 13/30\n",
      "502/502 - 5s - loss: 13.5646 - accuracy: 0.1826 - val_loss: 9.5754 - val_accuracy: 0.2008 - 5s/epoch - 10ms/step\n",
      "Epoch 14/30\n",
      "502/502 - 5s - loss: 13.3399 - accuracy: 0.1826 - val_loss: 9.4650 - val_accuracy: 0.2008 - 5s/epoch - 11ms/step\n",
      "Epoch 15/30\n",
      "502/502 - 5s - loss: 13.1377 - accuracy: 0.1827 - val_loss: 9.3522 - val_accuracy: 0.1993 - 5s/epoch - 11ms/step\n",
      "Epoch 16/30\n",
      "502/502 - 7s - loss: 12.9363 - accuracy: 0.1802 - val_loss: 9.2569 - val_accuracy: 0.1985 - 7s/epoch - 14ms/step\n",
      "Epoch 17/30\n",
      "502/502 - 7s - loss: 12.7316 - accuracy: 0.1807 - val_loss: 9.1411 - val_accuracy: 0.1975 - 7s/epoch - 15ms/step\n",
      "Epoch 18/30\n",
      "502/502 - 7s - loss: 15.9741 - accuracy: 0.1758 - val_loss: 9.3669 - val_accuracy: 0.1757 - 7s/epoch - 15ms/step\n",
      "Epoch 19/30\n",
      "502/502 - 8s - loss: 34.2719 - accuracy: 0.1889 - val_loss: 10.4611 - val_accuracy: 0.2364 - 8s/epoch - 15ms/step\n",
      "Epoch 20/30\n",
      "502/502 - 8s - loss: 36.9417 - accuracy: 0.1830 - val_loss: 10.3304 - val_accuracy: 0.2484 - 8s/epoch - 15ms/step\n",
      "Epoch 21/30\n",
      "502/502 - 7s - loss: 37.2749 - accuracy: 0.1968 - val_loss: 10.2656 - val_accuracy: 0.3557 - 7s/epoch - 15ms/step\n",
      "Epoch 22/30\n",
      "502/502 - 8s - loss: 37.9426 - accuracy: 0.2193 - val_loss: 10.2692 - val_accuracy: 0.4121 - 8s/epoch - 15ms/step\n",
      "Epoch 23/30\n",
      "502/502 - 8s - loss: 38.9384 - accuracy: 0.2093 - val_loss: 10.3427 - val_accuracy: 0.3933 - 8s/epoch - 15ms/step\n",
      "Epoch 24/30\n",
      "502/502 - 8s - loss: 40.1423 - accuracy: 0.2006 - val_loss: 10.4577 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "Epoch 25/30\n",
      "502/502 - 8s - loss: 41.2835 - accuracy: 0.2000 - val_loss: 10.6525 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "Epoch 26/30\n",
      "502/502 - 8s - loss: 42.2037 - accuracy: 0.2000 - val_loss: 10.8148 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "Epoch 27/30\n",
      "502/502 - 8s - loss: 42.9944 - accuracy: 0.2000 - val_loss: 10.9510 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "Epoch 28/30\n",
      "502/502 - 8s - loss: 43.5723 - accuracy: 0.2000 - val_loss: 11.0720 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "Epoch 29/30\n",
      "502/502 - 7s - loss: 44.0799 - accuracy: 0.2000 - val_loss: 11.1741 - val_accuracy: 0.3911 - 7s/epoch - 15ms/step\n",
      "Epoch 30/30\n",
      "502/502 - 8s - loss: 44.3807 - accuracy: 0.2000 - val_loss: 11.2605 - val_accuracy: 0.3911 - 8s/epoch - 15ms/step\n",
      "\n",
      "4. 模型评估...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.70       816\n",
      "           1       0.57      0.55      0.56      1560\n",
      "           2       0.34      0.58      0.42      1009\n",
      "           3       0.00      0.00      0.00       495\n",
      "           4       0.00      0.00      0.00       109\n",
      "\n",
      "    accuracy                           0.50      3989\n",
      "   macro avg       0.33      0.36      0.34      3989\n",
      "weighted avg       0.46      0.50      0.47      3989\n",
      "\n",
      "MAE: 0.5700676861368764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "from scipy.stats import bootstrap\n",
    "import itertools\n",
    "\n",
    "# ============================= T2D-痴呆结果 Gradually_Increasing =============================\n",
    "\n",
    "# =============================== 配置区 ===============================\n",
    "DATA_PATH = \"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Gradually_Increasing.csv\"  # 修改为实际路径\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "SHAP_SAMPLE_SIZE = 500  # SHAP计算样本量\n",
    "LEARNING_RATE = 1e-5  # 调低学习率\n",
    "\n",
    "# ============================= 1.数据准备 =============================\n",
    "print(\"\\n1. 数据加载与预处理...\")\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "protein_names = df.columns[9:417].tolist()\n",
    "covariates_names = df.columns[2:9].tolist()\n",
    "num_proteins = len(protein_names)\n",
    "\n",
    "# 数据拆分\n",
    "y = df.iloc[:, 1].values.astype(np.int32)\n",
    "X_protein = df.iloc[:, 9:417].values.astype(np.float32)\n",
    "X_covariates = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# 缺失值处理\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_protein = imputer.fit_transform(X_protein)\n",
    "X_covariates = imputer.fit_transform(X_covariates)\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_protein_scaled = scaler.fit_transform(X_protein)\n",
    "X_covariates_scaled = scaler.fit_transform(X_covariates)\n",
    "X_combined = np.hstack([X_protein_scaled, X_covariates_scaled])\n",
    "\n",
    "# 数据集拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# SMOTE过采样\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Further split training data into sub-train and validation for grid search\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define grid search parameters\n",
    "layer_configs = [[1024, 512, 256, 128], [512, 256, 128], [256, 128]]  # Varying complexity\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "l2_lambdas = [0.001, 0.01, 0.1]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Function to build model with variable hyperparameters\n",
    "class MonotonicConstraint(tf.keras.constraints.Constraint):\n",
    "    \"\"\"强制阈值参数单调递增约束\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.cumsum(tf.nn.elu(w) + 1e-6)\n",
    "\n",
    "def build_model(input_dim, num_classes, layer_sizes, dropout_rate, l2_lambda, learning_rate):\n",
    "    class DeepOrdinal(tf.keras.Model):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # 网络结构\n",
    "            self.dense_stack = tf.keras.Sequential()\n",
    "            for size in layer_sizes:\n",
    "                self.dense_stack.add(tf.keras.layers.Dense(size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "                self.dense_stack.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "            self.output_layer = tf.keras.layers.Dense(num_classes-1, kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))\n",
    "            \n",
    "            # 可训练阈值参数\n",
    "            self.thresholds = tf.Variable(\n",
    "                initial_value=tf.sort(tf.linspace(-1.0, 1.0, num_classes-1)),\n",
    "                trainable=True,\n",
    "                constraint=MonotonicConstraint(),\n",
    "                name=\"ordinal_thresholds\"\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_stack(inputs)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            \"\"\"序数回归损失函数\"\"\"\n",
    "            y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)\n",
    "            cum_loss = 0.0\n",
    "            \n",
    "            for k in range(self.num_classes-1):\n",
    "                target = tf.cast(y_true > k, tf.float32)\n",
    "                logit = y_pred[:, k] - self.thresholds[k]\n",
    "                \n",
    "                # 添加数值稳定化\n",
    "                logit = tf.clip_by_value(logit, -10.0, 10.0)\n",
    "                logit = tf.reshape(logit, (-1, 1))\n",
    "                \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(target, logit)\n",
    "                cum_loss += tf.reduce_mean(loss)\n",
    "                \n",
    "            return cum_loss\n",
    "\n",
    "    model = DeepOrdinal(input_dim=input_dim, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipvalue=1.0  # 梯度裁剪\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.custom_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "best_score = float('inf')  # Minimize validation MAE\n",
    "best_params = None\n",
    "results = []  # Track all configurations\n",
    "\n",
    "# 类别权重（根据数据分布调整）\n",
    "class_weight = {0: 1.0, 1: 2.5, 2: 3.0, 3: 5.0, 4: 10.0}\n",
    "\n",
    "for layer_sizes, dropout_rate, l2_lambda, lr in itertools.product(layer_configs, dropout_rates, l2_lambdas, learning_rates):\n",
    "    print(f\"Evaluating config: layers={layer_sizes}, dropout={dropout_rate}, l2={l2_lambda}, lr={lr}\")\n",
    "    \n",
    "    model = build_model(X_subtrain.shape[1], num_classes=5, layer_sizes=layer_sizes, \n",
    "                        dropout_rate=dropout_rate, l2_lambda=l2_lambda, learning_rate=lr)\n",
    "    \n",
    "    # Train with class weights\n",
    "    history = model.fit(\n",
    "        X_subtrain, y_subtrain,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weight,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict on validation and compute MAE\n",
    "    def ordinal_predict(model, X):\n",
    "        \"\"\"将原始输出转换为有序类别\"\"\"\n",
    "        raw_output = model.predict(X, verbose=0)\n",
    "        thresholds = model.thresholds.numpy()\n",
    "        cum_probs = tf.sigmoid(raw_output - thresholds)\n",
    "        binary_probs = tf.cast(cum_probs > 0.5, tf.int32)\n",
    "        return tf.reduce_sum(binary_probs, axis=1).numpy()\n",
    "\n",
    "    y_val_pred = ordinal_predict(model, X_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr, 'val_mae': val_mae\n",
    "    })\n",
    "    \n",
    "    if val_mae < best_score:\n",
    "        best_score = val_mae\n",
    "        best_params = {'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr}\n",
    "\n",
    "# Save results to CSV for review\n",
    "pd.DataFrame(results).to_csv('E:/neuro_od/T2D-痴呆结果/grid_search_results.csv', index=False)\n",
    "\n",
    "print(f\"Best configuration: {best_params} with validation MAE: {best_score}\")\n",
    "\n",
    "# Now, use best_params to build and train the final model on full X_train, y_train\n",
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=5,\n",
    "    layer_sizes=best_params['layers'],\n",
    "    dropout_rate=best_params['dropout'],\n",
    "    l2_lambda=best_params['l2'],\n",
    "    learning_rate=best_params['lr']\n",
    ")\n",
    "\n",
    "# ============================ 3.模型训练 =============================\n",
    "print(\"\\n3. 模型训练中...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# =========================== 4.模型评估 =============================\n",
    "print(\"\\n4. 模型评估...\")\n",
    "\n",
    "y_pred = ordinal_predict(model, X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a02cb-f5cc-43d1-b554-1d769da5cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5.梯度重要性计算（蛋白质部分）=====================\n",
    "print(\"\\n5. 计算梯度重要性...\")\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)  # 显式监视输入\n",
    "        preds = model(inputs)\n",
    "    return tape.gradient(preds, inputs)\n",
    "\n",
    "# 计算所有训练样本的梯度均值\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "gradients = compute_gradients(X_train_tensor)\n",
    "\n",
    "if gradients is None:\n",
    "    raise ValueError(\"梯度计算失败，请检查模型输入输出依赖关系\")\n",
    "\n",
    "# 调整聚合维度（假设输入为二维张量）\n",
    "abs_gradients = tf.abs(gradients).numpy()  # Convert to NumPy for bootstrapping\n",
    "protein_abs_gradients = abs_gradients[:, :num_proteins]  # Extract protein part (first num_proteins columns)\n",
    "\n",
    "# Bootstrapping function for mean importance per protein\n",
    "def bootstrap_mean(data):\n",
    "    return np.mean(data, axis=0)  # Mean across samples for each protein\n",
    "\n",
    "# Perform bootstrapping (95% CI, 1000 resamples)\n",
    "boot_result = bootstrap((protein_abs_gradients,), bootstrap_mean, n_resamples=1000, random_state=RANDOM_STATE, method='percentile')\n",
    "\n",
    "# Extract statistics\n",
    "gradient_importance_mean = boot_result.bootstrap_distribution.mean(axis=-1)  # Mean of bootstraps across resamples; results in shape (67,)\n",
    "gradient_importance_se = boot_result.standard_error\n",
    "gradient_importance_ci_low = boot_result.confidence_interval.low\n",
    "gradient_importance_ci_high = boot_result.confidence_interval.high\n",
    "\n",
    "# Diagnostic prints to verify lengths (optional; can be removed after confirmation)\n",
    "print(\"Length of protein_names:\", len(protein_names))\n",
    "print(\"Shape of gradient_importance_mean:\", gradient_importance_mean.shape)\n",
    "print(\"Shape of gradient_importance_se:\", gradient_importance_se.shape)\n",
    "print(\"Shape of gradient_importance_ci_low:\", gradient_importance_ci_low.shape)\n",
    "print(\"Shape of gradient_importance_ci_high:\", gradient_importance_ci_high.shape)\n",
    "\n",
    "# Save results to CSV\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': gradient_importance_mean,\n",
    "    'Gradient Importance SE': gradient_importance_se,\n",
    "    'Gradient Importance CI Lower (95%)': gradient_importance_ci_low,\n",
    "    'Gradient Importance CI Upper (95%)': gradient_importance_ci_high\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_dementia_Gradually_Increasing_深度有序回归_with_ci.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1273b-7cbc-470d-adda-08f079f36339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Permutation Test\n",
    "print(\"\\n5.1. Performing Permutation Test...\")\n",
    "n_permutations = 1000  # Adjust for computational feasibility\n",
    "num_covariates = len(covariates_names)  # e.g., 68 in this run\n",
    "start_index = num_covariates  # Assume covariates first, proteins next; adjust to 0 if proteins are first\n",
    "\n",
    "def compute_mean_abs_gradients(inputs):\n",
    "    grads = compute_gradients(tf.convert_to_tensor(inputs, dtype=tf.float32))\n",
    "    return np.mean(np.abs(grads.numpy()), axis=0)[start_index:start_index + num_proteins]  # Extract protein part only\n",
    "\n",
    "# Extract protein gradients consistently\n",
    "protein_abs_gradients = abs_gradients[:, start_index:start_index + num_proteins]\n",
    "\n",
    "# Verify data consistency\n",
    "print(\"Number of proteins (num_proteins):\", num_proteins)\n",
    "print(\"Length of protein_names:\", len(protein_names))\n",
    "print(\"Number of covariates:\", num_covariates)\n",
    "print(\"Start index for proteins:\", start_index)\n",
    "print(\"Shape of protein_abs_gradients:\", protein_abs_gradients.shape)\n",
    "\n",
    "# Compute original mean importance for proteins\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "print(\"Shape of original_mean:\", original_mean.shape)\n",
    "\n",
    "# Generate null distribution by permuting protein columns only\n",
    "null_distribution = np.zeros((n_permutations, num_proteins))\n",
    "for i in range(n_permutations):\n",
    "    permuted_X = X_train.copy()\n",
    "    for j in range(start_index, start_index + num_proteins):\n",
    "        np.random.shuffle(permuted_X[:, j])\n",
    "    null_distribution[i] = compute_mean_abs_gradients(permuted_X)\n",
    "print(\"Shape of null_distribution:\", null_distribution.shape)\n",
    "\n",
    "# Compute p-values\n",
    "p_values_perm = np.array([1 - (percentileofscore(null_distribution[:, j], original_mean[j]) / 100) for j in range(num_proteins)])\n",
    "print(\"Length of p_values_perm:\", len(p_values_perm))\n",
    "\n",
    "# Verify lengths before DataFrame creation\n",
    "if not (len(protein_names) == len(original_mean) == len(p_values_perm)):\n",
    "    raise ValueError(f\"Length mismatch: protein_names ({len(protein_names)}), original_mean ({len(original_mean)}), p_values_perm ({len(p_values_perm)})\")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': original_mean,\n",
    "    'P-value (Permutation)': p_values_perm\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_permutation_test_Gradually_Increasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232a0ab-0541-4046-a360-93cc67403fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified; insert after model training\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Approximate pivotal statistic (e.g., standardized gradient mean)\n",
    "std_gradients = np.std(protein_abs_gradients, axis=0)\n",
    "pivotal_stats = original_mean / (std_gradients / np.sqrt(protein_abs_gradients.shape[0]))\n",
    "p_values_pivotal = 2 * (1 - norm.cdf(np.abs(pivotal_stats)))  # Two-tailed\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Pivotal Statistic': pivotal_stats,\n",
    "    'P-value (Pivotal)': p_values_pivotal\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_pivotal_test_Gradually_Increasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7063dc28-5989-43f5-8923-01b2e53322ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 数据加载与预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HTS\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[1024, 512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[512, 256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.2, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.3, l2=0.1, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.001, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.01, lr=0.0001\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=1e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=5e-05\n",
      "Evaluating config: layers=[256, 128], dropout=0.4, l2=0.1, lr=0.0001\n",
      "Best configuration: {'layers': [1024, 512, 256, 128], 'dropout': 0.3, 'l2': 0.01, 'lr': 0.0001} with validation MAE: 0.39376947040498445\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "build_model() got an unexpected keyword argument 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 167\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest configuration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with validation MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Now, use best_params to build and train the final model on full X_train, y_train\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# ============================ 3.模型训练 =============================\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. 模型训练中...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: build_model() got an unexpected keyword argument 'layers'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================= T2D-痴呆结果 Gradually_Increasing =============================\n",
    "\n",
    "# =============================== 配置区 ===============================\n",
    "DATA_PATH = \"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Gradually_Decreasing.csv\"  # 修改为实际路径\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "SHAP_SAMPLE_SIZE = 500  # SHAP计算样本量\n",
    "LEARNING_RATE = 1e-5  # 调低学习率\n",
    "\n",
    "# ============================= 1.数据准备 =============================\n",
    "print(\"\\n1. 数据加载与预处理...\")\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"E:/neuro_od/T2D-痴呆结果/extracted_data/extracted_Gradually_Decreasing.csv\")\n",
    "protein_names = df.columns[9:63].tolist()\n",
    "covariates_names = df.columns[2:9].tolist()\n",
    "num_proteins = len(protein_names)\n",
    "\n",
    "# 数据拆分\n",
    "y = df.iloc[:, 1].values.astype(np.int32)\n",
    "X_protein = df.iloc[:, 9:63].values.astype(np.float32)\n",
    "X_covariates = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# 缺失值处理\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_protein = imputer.fit_transform(X_protein)\n",
    "X_covariates = imputer.fit_transform(X_covariates)\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_protein_scaled = scaler.fit_transform(X_protein)\n",
    "X_covariates_scaled = scaler.fit_transform(X_covariates)\n",
    "X_combined = np.hstack([X_protein_scaled, X_covariates_scaled])\n",
    "\n",
    "# 数据集拆分\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# SMOTE过采样\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Further split training data into sub-train and validation for grid search\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define grid search parameters\n",
    "layer_configs = [[1024, 512, 256, 128], [512, 256, 128], [256, 128]]  # Varying complexity\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "l2_lambdas = [0.001, 0.01, 0.1]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Function to build model with variable hyperparameters\n",
    "class MonotonicConstraint(tf.keras.constraints.Constraint):\n",
    "    \"\"\"强制阈值参数单调递增约束\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.cumsum(tf.nn.elu(w) + 1e-6)\n",
    "\n",
    "def build_model(input_dim, num_classes, layer_sizes, dropout_rate, l2_lambda, learning_rate):\n",
    "    class DeepOrdinal(tf.keras.Model):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # 网络结构\n",
    "            self.dense_stack = tf.keras.Sequential()\n",
    "            for size in layer_sizes:\n",
    "                self.dense_stack.add(tf.keras.layers.Dense(size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "                self.dense_stack.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "            self.output_layer = tf.keras.layers.Dense(num_classes-1, kernel_regularizer=tf.keras.regularizers.l2(l2_lambda))\n",
    "            \n",
    "            # 可训练阈值参数\n",
    "            self.thresholds = tf.Variable(\n",
    "                initial_value=tf.sort(tf.linspace(-1.0, 1.0, num_classes-1)),\n",
    "                trainable=True,\n",
    "                constraint=MonotonicConstraint(),\n",
    "                name=\"ordinal_thresholds\"\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_stack(inputs)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            \"\"\"序数回归损失函数\"\"\"\n",
    "            y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)\n",
    "            cum_loss = 0.0\n",
    "            \n",
    "            for k in range(self.num_classes-1):\n",
    "                target = tf.cast(y_true > k, tf.float32)\n",
    "                logit = y_pred[:, k] - self.thresholds[k]\n",
    "                \n",
    "                # 添加数值稳定化\n",
    "                logit = tf.clip_by_value(logit, -10.0, 10.0)\n",
    "                logit = tf.reshape(logit, (-1, 1))\n",
    "                \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(target, logit)\n",
    "                cum_loss += tf.reduce_mean(loss)\n",
    "                \n",
    "            return cum_loss\n",
    "\n",
    "    model = DeepOrdinal(input_dim=input_dim, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipvalue=1.0  # 梯度裁剪\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.custom_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "best_score = float('inf')  # Minimize validation MAE\n",
    "best_params = None\n",
    "results = []  # Track all configurations\n",
    "\n",
    "# 类别权重（根据数据分布调整）\n",
    "class_weight = {0: 1.0, 1: 2.5, 2: 3.0, 3: 5.0, 4: 10.0}\n",
    "\n",
    "for layer_sizes, dropout_rate, l2_lambda, lr in itertools.product(layer_configs, dropout_rates, l2_lambdas, learning_rates):\n",
    "    print(f\"Evaluating config: layers={layer_sizes}, dropout={dropout_rate}, l2={l2_lambda}, lr={lr}\")\n",
    "    \n",
    "    model = build_model(X_subtrain.shape[1], num_classes=5, layer_sizes=layer_sizes, \n",
    "                        dropout_rate=dropout_rate, l2_lambda=l2_lambda, learning_rate=lr)\n",
    "    \n",
    "    # Train with class weights\n",
    "    history = model.fit(\n",
    "        X_subtrain, y_subtrain,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weight,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict on validation and compute MAE\n",
    "    def ordinal_predict(model, X):\n",
    "        \"\"\"将原始输出转换为有序类别\"\"\"\n",
    "        raw_output = model.predict(X, verbose=0)\n",
    "        thresholds = model.thresholds.numpy()\n",
    "        cum_probs = tf.sigmoid(raw_output - thresholds)\n",
    "        binary_probs = tf.cast(cum_probs > 0.5, tf.int32)\n",
    "        return tf.reduce_sum(binary_probs, axis=1).numpy()\n",
    "\n",
    "    y_val_pred = ordinal_predict(model, X_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr, 'val_mae': val_mae\n",
    "    })\n",
    "    \n",
    "    if val_mae < best_score:\n",
    "        best_score = val_mae\n",
    "        best_params = {'layers': layer_sizes, 'dropout': dropout_rate, 'l2': l2_lambda, 'lr': lr}\n",
    "\n",
    "# Save results to CSV for review\n",
    "pd.DataFrame(results).to_csv('E:/neuro_od/T2D-痴呆结果/grid_search_results.csv', index=False)\n",
    "\n",
    "print(f\"Best configuration: {best_params} with validation MAE: {best_score}\")\n",
    "\n",
    "# Now, use best_params to build and train the final model on full X_train, y_train\n",
    "model = build_model(X_train.shape[1], num_classes=5, **best_params)\n",
    "\n",
    "# ============================ 3.模型训练 =============================\n",
    "print(\"\\n3. 模型训练中...\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# =========================== 4.模型评估 =============================\n",
    "print(\"\\n4. 模型评估...\")\n",
    "\n",
    "y_pred = ordinal_predict(model, X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ce956-dfbc-436d-8ae1-950b260f76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5.梯度重要性计算（蛋白质部分）=====================\n",
    "print(\"\\n5. 计算梯度重要性...\")\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)  # 显式监视输入\n",
    "        preds = model(inputs)\n",
    "    return tape.gradient(preds, inputs)\n",
    "\n",
    "# 计算所有训练样本的梯度均值\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "gradients = compute_gradients(X_train_tensor)\n",
    "\n",
    "if gradients is None:\n",
    "    raise ValueError(\"梯度计算失败，请检查模型输入输出依赖关系\")\n",
    "\n",
    "# 调整聚合维度（假设输入为二维张量）\n",
    "abs_gradients = tf.abs(gradients).numpy()  # Convert to NumPy for bootstrapping\n",
    "protein_abs_gradients = abs_gradients[:, :num_proteins]  # Extract protein part (first num_proteins columns)\n",
    "\n",
    "# Bootstrapping function for mean importance per protein\n",
    "def bootstrap_mean(data):\n",
    "    return np.mean(data, axis=0)  # Mean across samples for each protein\n",
    "\n",
    "# Perform bootstrapping (95% CI, 1000 resamples)\n",
    "boot_result = bootstrap((protein_abs_gradients,), bootstrap_mean, n_resamples=1000, random_state=RANDOM_STATE, method='percentile')\n",
    "\n",
    "# Extract statistics\n",
    "gradient_importance_mean = boot_result.bootstrap_distribution.mean(axis=-1)  # Mean of bootstraps across resamples; results in shape (67,)\n",
    "gradient_importance_se = boot_result.standard_error\n",
    "gradient_importance_ci_low = boot_result.confidence_interval.low\n",
    "gradient_importance_ci_high = boot_result.confidence_interval.high\n",
    "\n",
    "# Diagnostic prints to verify lengths (optional; can be removed after confirmation)\n",
    "print(\"Length of protein_names:\", len(protein_names))\n",
    "print(\"Shape of gradient_importance_mean:\", gradient_importance_mean.shape)\n",
    "print(\"Shape of gradient_importance_se:\", gradient_importance_se.shape)\n",
    "print(\"Shape of gradient_importance_ci_low:\", gradient_importance_ci_low.shape)\n",
    "print(\"Shape of gradient_importance_ci_high:\", gradient_importance_ci_high.shape)\n",
    "\n",
    "# Save results to CSV\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': gradient_importance_mean,\n",
    "    'Gradient Importance SE': gradient_importance_se,\n",
    "    'Gradient Importance CI Lower (95%)': gradient_importance_ci_low,\n",
    "    'Gradient Importance CI Upper (95%)': gradient_importance_ci_high\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_dementia_Gradually_decreasing_深度有序回归_with_ci.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809a9ed-64a0-4c53-a0ba-d891b689245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Permutation Test Parameters\n",
    "n_permutations = 1000  # Adjust for computational feasibility\n",
    "\n",
    "# Function to compute mean absolute gradients\n",
    "def compute_mean_abs_gradients(inputs):\n",
    "    grads = compute_gradients(tf.convert_to_tensor(inputs, dtype=tf.float32))\n",
    "    return np.mean(np.abs(grads.numpy()), axis=0)[:num_proteins]  # Protein part only\n",
    "\n",
    "# Original mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Generate null distribution\n",
    "null_distribution = np.zeros((n_permutations, num_proteins))\n",
    "for i in range(n_permutations):\n",
    "    # Permute features (columns) independently for each protein\n",
    "    permuted_X = X_train.copy()\n",
    "    for j in range(num_proteins):\n",
    "        np.random.shuffle(permuted_X[:, j])\n",
    "    null_distribution[i] = compute_mean_abs_gradients(permuted_X)\n",
    "\n",
    "# Compute p-values (one-tailed: proportion of null means >= original)\n",
    "p_values_perm = np.array([1 - (percentileofscore(null_distribution[:, j], original_mean[j]) / 100) for j in range(num_proteins)])\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': original_mean,\n",
    "    'P-value (Permutation)': p_values_perm\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_permutation_test_Gradually_Decreasing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61844e1-3af2-4e34-9415-7c631ee65698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified; insert after model training\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Approximate pivotal statistic (e.g., standardized gradient mean)\n",
    "std_gradients = np.std(protein_abs_gradients, axis=0)\n",
    "pivotal_stats = original_mean / (std_gradients / np.sqrt(protein_abs_gradients.shape[0]))\n",
    "p_values_pivotal = 2 * (1 - norm.cdf(np.abs(pivotal_stats)))  # Two-tailed\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Pivotal Statistic': pivotal_stats,\n",
    "    'P-value (Pivotal)': p_values_pivotal\n",
    "}).to_csv('E:/neuro_od/T2D-痴呆结果/gradient_importance_pivotal_test_Gradually_Decreasing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
